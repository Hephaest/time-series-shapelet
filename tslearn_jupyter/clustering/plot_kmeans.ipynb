{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nk-means\n=======\n\nThis example uses $k$-means clustering for time series. Three variants of\nthe algorithm are available: standard\nEuclidean $k$-means, DBA-$k$-means (for DTW Barycenter\nAveraging [1])\nand Soft-DTW $k$-means [2].\n\nIn the figure below, each row corresponds to the result of a different\nclustering. In a row, each sub-figure corresponds to a cluster.\nIt represents the set\nof time series from the training set that were assigned to the considered\ncluster (in black) as well as the barycenter of the cluster (in red).\n\nA note on pre-processing\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn this example, time series are preprocessed using\n`TimeSeriesScalerMeanVariance`. This scaler is such that each output time\nseries has zero mean and unit variance.\nThe assumption here is that the range of a given time series is uninformative\nand one only wants to compare shapes in an amplitude-invariant manner (when\ntime series are multivariate, this also rescales all modalities such that there\nwill not be a single modality responsible for a large part of the variance).\nThis means that one cannot scale barycenters back to data range because each\ntime series is scaled independently and there is hence no such thing as an\noverall data range.\n\n[1] F. Petitjean, A. Ketterlin & P. Gancarski. A global averaging method for dynamic time warping, with applications to clustering. Pattern Recognition, Elsevier, 2011, Vol. 44, Num. 3, pp. 678-693\n[2] M. Cuturi, M. Blondel \"Soft-DTW: a Differentiable Loss Function for Time-Series,\" ICML 2017.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Romain Tavenard\n# License: BSD 3 clause\n\nimport numpy\nimport matplotlib.pyplot as plt\n\nfrom tslearn.clustering import TimeSeriesKMeans\nfrom tslearn.datasets import CachedDatasets\nfrom tslearn.preprocessing import TimeSeriesScalerMeanVariance, \\\n    TimeSeriesResampler\n\nseed = 0\nnumpy.random.seed(seed)\nX_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\nX_train = X_train[y_train < 4]  # Keep first 3 classes\nnumpy.random.shuffle(X_train)\n# Keep only 50 time series\nX_train = TimeSeriesScalerMeanVariance().fit_transform(X_train[:50])\n# Make time series shorter\nX_train = TimeSeriesResampler(sz=40).fit_transform(X_train)\nsz = X_train.shape[1]\n\n# Euclidean k-means\nprint(\"Euclidean k-means\")\nkm = TimeSeriesKMeans(n_clusters=3, verbose=True, random_state=seed)\ny_pred = km.fit_predict(X_train)\n\nplt.figure()\nfor yi in range(3):\n    plt.subplot(3, 3, yi + 1)\n    for xx in X_train[y_pred == yi]:\n        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n    plt.plot(km.cluster_centers_[yi].ravel(), \"r-\")\n    plt.xlim(0, sz)\n    plt.ylim(-4, 4)\n    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n             transform=plt.gca().transAxes)\n    if yi == 1:\n        plt.title(\"Euclidean $k$-means\")\n\n# DBA-k-means\nprint(\"DBA k-means\")\ndba_km = TimeSeriesKMeans(n_clusters=3,\n                          n_init=2,\n                          metric=\"dtw\",\n                          verbose=True,\n                          max_iter_barycenter=10,\n                          random_state=seed)\ny_pred = dba_km.fit_predict(X_train)\n\nfor yi in range(3):\n    plt.subplot(3, 3, 4 + yi)\n    for xx in X_train[y_pred == yi]:\n        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n    plt.plot(dba_km.cluster_centers_[yi].ravel(), \"r-\")\n    plt.xlim(0, sz)\n    plt.ylim(-4, 4)\n    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n             transform=plt.gca().transAxes)\n    if yi == 1:\n        plt.title(\"DBA $k$-means\")\n\n# Soft-DTW-k-means\nprint(\"Soft-DTW k-means\")\nsdtw_km = TimeSeriesKMeans(n_clusters=3,\n                           metric=\"softdtw\",\n                           metric_params={\"gamma\": .01},\n                           verbose=True,\n                           random_state=seed)\ny_pred = sdtw_km.fit_predict(X_train)\n\nfor yi in range(3):\n    plt.subplot(3, 3, 7 + yi)\n    for xx in X_train[y_pred == yi]:\n        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n    plt.plot(sdtw_km.cluster_centers_[yi].ravel(), \"r-\")\n    plt.xlim(0, sz)\n    plt.ylim(-4, 4)\n    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n             transform=plt.gca().transAxes)\n    if yi == 1:\n        plt.title(\"Soft-DTW $k$-means\")\n\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}