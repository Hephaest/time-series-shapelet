{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLearning Shapelets\n==================\n\nThis example illustrates how the \"Learning Shapelets\" method can quickly\nfind a set of shapelets that results in excellent predictive performance\nwhen used for a shapelet transform.\n\nMore information on the method can be found at:\nhttp://fs.ismll.de/publicspace/LearningShapelets/.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Romain Tavenard\n# License: BSD 3 clause\n\nimport numpy\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tslearn.datasets import CachedDatasets\nfrom tslearn.preprocessing import TimeSeriesScalerMinMax\nfrom tslearn.shapelets import LearningShapelets, \\\n    grabocka_params_to_shapelet_size_dict\nfrom tslearn.utils import ts_size\n\n# Set seed for determinism\nnumpy.random.seed(0)\n\n# Load the Trace dataset\nX_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n\n# Normalize each of the timeseries in the Trace dataset\nX_train = TimeSeriesScalerMinMax().fit_transform(X_train)\nX_test = TimeSeriesScalerMinMax().fit_transform(X_test)\n\n# Get statistics of the dataset\nn_ts, ts_sz = X_train.shape[:2]\nn_classes = len(set(y_train))\n\n# Set the number of shapelets per size as done in the original paper\nshapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=n_ts,\n                                                       ts_sz=ts_sz,\n                                                       n_classes=n_classes,\n                                                       l=0.1,\n                                                       r=1)\n\n# Define the model using parameters provided by the authors (except that we\n# use fewer iterations here)\nshp_clf = LearningShapelets(n_shapelets_per_size=shapelet_sizes,\n                            optimizer=tf.optimizers.Adam(.01),\n                            batch_size=16,\n                            weight_regularizer=.01,\n                            max_iter=200,\n                            random_state=42,\n                            verbose=0)\nshp_clf.fit(X_train, y_train)\n\n# Make predictions and calculate accuracy score\npred_labels = shp_clf.predict(X_test)\nprint(\"Correct classification rate:\", accuracy_score(y_test, pred_labels))\n\n# Plot the different discovered shapelets\nplt.figure()\nfor i, sz in enumerate(shapelet_sizes.keys()):\n    plt.subplot(len(shapelet_sizes), 1, i + 1)\n    plt.title(\"%d shapelets of size %d\" % (shapelet_sizes[sz], sz))\n    for shp in shp_clf.shapelets_:\n        if ts_size(shp) == sz:\n            plt.plot(shp.ravel())\n    plt.xlim([0, max(shapelet_sizes.keys()) - 1])\n\nplt.tight_layout()\nplt.show()\n\n# The loss history is accessible via the `model_` that is a keras model\nplt.figure()\nplt.plot(numpy.arange(1, shp_clf.n_iter_ + 1), shp_clf.history_[\"loss\"])\nplt.title(\"Evolution of cross-entropy loss during training\")\nplt.xlabel(\"Epochs\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}